# ğŸš€ Next Word Prediction using LSTM & GRU

A lightweight NLP project that predicts the next word in a sentence using deep learning models (LSTM & GRU). The project includes data cleaning, tokenization, model training, and a Streamlit web app for real-time auto-complete suggestions.

---

## ğŸ“ Project Structure

```
next-word-predictor/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.txt
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ lstm_model.keras
â”‚   â”œâ”€â”€ gru_model.keras
â”‚   â””â”€â”€ tokenizer.pkl
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ lstm_training.ipynb
â”œâ”€â”€ gru_training.ipynb
â””â”€â”€ README.md
```

---

## ğŸ§  Models Included

### **1ï¸âƒ£ LSTM-based Next Word Predictor**

* Trained using TensorFlow/Keras
* Uses Embedding + LSTM + Dense
* Predicts next word for any given input text
* Handles unknown words using OOV token

### **2ï¸âƒ£ GRU-based Next Word Predictor**

* Faster training
* Requires fewer parameters
* Produces competitive accuracy
* Integrated with the same tokenizer for consistency

---

## âš™ï¸ Tech Stack

| Component         | Technology                       |
| ----------------- | -------------------------------- |
| Model Training    | Python, TensorFlow/Keras         |
| Data Processing   | NumPy, Regex, Tokenizer          |
| Frontend UI       | Streamlit                        |
| Model Storage     | `.keras` (TensorFlow SavedModel) |
| Tokenizer Storage | `.pkl` (Pickle)                  |

---

## ğŸ”§ Training the Models

### **Step 1 â€” Clean & Preprocess Data**

Both LSTM & GRU notebooks include:

* Lowercasing
* Removing punctuation
* Removing special characters
* Tokenization
* Generating input sequences
* Creating n-grams
* One-hot encoding labels

### **Step 2 â€” Train LSTM**

```python
model = Sequential()
model.add(Embedding(vocab_size, 128, input_length=max_len-1))
model.add(LSTM(150))
model.add(Dense(vocab_size, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adam")
model.fit(X, y, epochs=100, batch_size=32)
model.save("model/lstm_model.keras")
```

### **Step 3 â€” Train GRU**

```python
model = Sequential()
model.add(Embedding(vocab_size, 128, input_length=max_len-1))
model.add(GRU(150))
model.add(Dense(vocab_size, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adam")
model.fit(X, y, epochs=100, batch_size=32)
model.save("model/gru_model.keras")
```

---

## ğŸ’¾ Saving the Tokenizer

```python
import pickle

with open("model/tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)
```

---

## â–¶ï¸ Running the Web App

### **Install Dependencies**

```
pip install -r requirements.txt
```

### **Launch Streamlit**

```
streamlit run app.py
```

### Features

âœ” Auto-complete suggestions
âœ” Fetch suggestions without clicking a button
âœ” Loads LSTM or GRU model dynamically
âœ” Clean and minimal UI

---

## ğŸ§ª Model Prediction Logic

```python
def predict_next(word_sequence, top_k=4):
    tokens = tokenizer.texts_to_sequences([word_sequence])[0]
    tokens_padded = pad_sequences([tokens], maxlen=max_len-1, padding="pre")

    prediction = model.predict(tokens_padded)
    top_indices = prediction[0].argsort()[-top_k:][::-1]

    return [index_word[i] for i in top_indices]
```

---

## ğŸ“Œ Notes

* Ensure `model` folder contains `.keras` and `.pkl` files.
* Use same tokenizer for both LSTM and GRU.
* If file-not-found errors occur, check folder names carefully.
* Large dataset recommended for better predictions.

---

## ğŸ§‘â€ğŸ’» Author

**Guru**
Student & Developer
Working on ML, DL, and NLP projects.
